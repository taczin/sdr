{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BmEEmaem0om"
      },
      "outputs": [],
      "source": [
        "\"\"\"Top level file, parse flags and call trining loop.\"\"\"\n",
        "import os\n",
        "from utils.pytorch_lightning_utils.pytorch_lightning_utils import load_params_from_checkpoint\n",
        "import torch\n",
        "from pytorch_lightning.profiler.profilers import SimpleProfiler\n",
        "from utils.pytorch_lightning_utils.callbacks import RunValidationOnStart\n",
        "from utils import switch_functions\n",
        "import pytorch_lightning\n",
        "from pytorch_lightning import Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from utils.argparse_init import default_arg_parser, init_parse_argparse_default_params\n",
        "import logging\n",
        "from models.SDR.SDR import SDRDataset\n",
        "from models.SDR.CoherenceBaseline import CoherenceDataset\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "from pytorch_lightning.loggers import TensorBoardLogger\n",
        "\n",
        "def main():\n",
        "    \"\"\"Initialize all the parsers, before training init.\"\"\"\n",
        "    parser = default_arg_parser()\n",
        "    parser = Trainer.add_argparse_args(parser)  # Bug in PL\n",
        "    parser = default_arg_parser(description=\"docBert\", parents=[parser])\n",
        "\n",
        "    eager_flags = init_parse_argparse_default_params(parser)\n",
        "    model_class_pointer = switch_functions.model_class_pointer(eager_flags[\"task_name\"], eager_flags[\"architecture\"])\n",
        "    parser = model_class_pointer.add_model_specific_args(parser, eager_flags[\"task_name\"], eager_flags[\"dataset_name\"])\n",
        "\n",
        "    hyperparams = parser.parse_args()\n",
        "    main_train(model_class_pointer, hyperparams,parser)\n",
        "\n",
        "\n",
        "def main_train(model_class_pointer, hparams,parser):\n",
        "    \"\"\"Initialize the model, call training loop.\"\"\"\n",
        "    pytorch_lightning.utilities.seed.seed_everything(seed=hparams.seed)\n",
        "\n",
        "    if(hparams.resume_from_checkpoint not in [None,'']):\n",
        "        hparams = load_params_from_checkpoint(hparams, parser)\n",
        "\n",
        "    model = model_class_pointer(hparams)\n",
        "    if hparams.arch == \"CoherenceBaseline\":\n",
        "        sdr_dm = CoherenceDataset(hparams, model.tokenizer)\n",
        "    else:\n",
        "        sdr_dm = SDRDataset(hparams, model.tokenizer)\n",
        "\n",
        "    logger = TensorBoardLogger(save_dir=model.hparams.hparams_dir,name='',default_hp_metric=False)\n",
        "    logger.log_hyperparams(model.hparams, metrics={model.hparams.metric_to_track: 0})\n",
        "    print(f\"\\nLog directory:\\n{model.hparams.hparams_dir}\\n\")\n",
        "\n",
        "    trainer = pytorch_lightning.Trainer(\n",
        "        num_sanity_val_steps=2,\n",
        "        gradient_clip_val=hparams.max_grad_norm,\n",
        "        #callbacks=[RunValidationOnStart()],\n",
        "        checkpoint_callback=ModelCheckpoint(\n",
        "            save_top_k=3,\n",
        "            save_last=True,\n",
        "            mode=\"min\" if \"acc\" not in hparams.metric_to_track else \"max\",\n",
        "            monitor=hparams.metric_to_track,\n",
        "            dirpath=model.hparams.hparams_dir,\n",
        "            filename=\"{epoch}\",\n",
        "            verbose=True,\n",
        "        ),\n",
        "        logger=logger,\n",
        "        max_epochs=hparams.max_epochs,\n",
        "        gpus=hparams.gpus,\n",
        "        #strategy=\"ddp\",\n",
        "        limit_val_batches=hparams.limit_val_batches,\n",
        "        limit_train_batches=hparams.limit_train_batches,\n",
        "        limit_test_batches= hparams.limit_test_batches,\n",
        "        check_val_every_n_epoch=hparams.check_val_every_n_epoch,\n",
        "        profiler=SimpleProfiler(),\n",
        "        accumulate_grad_batches=hparams.accumulate_grad_batches,\n",
        "        reload_dataloaders_every_epoch=True,\n",
        "        resume_from_checkpoint=hparams.resume_from_checkpoint\n",
        "    )\n",
        "    if(not hparams.test_only):\n",
        "        trainer.fit(model, datamodule=sdr_dm)\n",
        "    else:\n",
        "        if(hparams.resume_from_checkpoint is not None):\n",
        "            model = model.load_from_checkpoint(hparams.resume_from_checkpoint,hparams=hparams, map_location=torch.device(f\"cpu\"))\n",
        "    trainer.test(model, sdr_dm)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    }
  ]
}